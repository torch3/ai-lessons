{"cells":[{"cell_type":"markdown","metadata":{"id":"QW1AKKHjCtSA"},"source":["# Обработка текста модулем `spacy`\n","\n","Простые функции обработки строк ничего не знают о тексте, о правилах языка. Но часто нужна более продвинутая обработка, например узнать все имена существительные в тексте. Для этого нужно использовать специальные библиотеки. Сегодня мы кратко рассмотрим две из них: `spacy` и `nltk`."]},{"cell_type":"markdown","metadata":{"id":"RngzSJ20CtSA"},"source":["Библиотека [__spacy__](https://nlpub.mipt.ru/SpaCy)  [(или здесь)](https://spacy.io/usage/spacy-101)\n","предназначена для обработки текстов на естественном языке (NLP, Natural Language Processing).\n","\n","Если она еще не установлена на компьютере - надо установить ее, дополнительную библиотеку [__textacy__](https://github.com/chartbeat-labs/textacy).\n","\n","Загрузим модель для английского языка, это займет время, ~1Гб:\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6861,"status":"ok","timestamp":1697639774480,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"},"user_tz":-180},"id":"vNDvMoB-VNvE","outputId":"dee5b471-c0f0-4f41-b622-148a11872f85"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"]}],"source":["# Установка spaCy\n","!pip3 install  spacy\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14485,"status":"ok","timestamp":1697639794019,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"},"user_tz":-180},"id":"4g_LKitcZDAw","outputId":"3e0c248a-ac54-418b-ae85-3c87ac2c05cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting textacy\n","  Downloading textacy-0.13.0-py3-none-any.whl (210 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (5.3.1)\n","Requirement already satisfied: catalogue~=2.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (2.0.10)\n","Collecting cytoolz>=0.10.1 (from textacy)\n","  Downloading cytoolz-0.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting floret~=0.10.0 (from textacy)\n","  Downloading floret-0.10.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (320 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.3/320.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jellyfish>=0.8.0 (from textacy)\n","  Downloading jellyfish-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.3.2)\n","Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from textacy) (3.1)\n","Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.23.5)\n","Collecting pyphen>=0.10.0 (from textacy)\n","  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (2.31.0)\n","Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.11.3)\n","Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.2.2)\n","Requirement already satisfied: spacy~=3.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (3.6.1)\n","Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.10/dist-packages (from textacy) (4.66.1)\n","Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from cytoolz>=0.10.1->textacy) (0.12.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (2023.7.22)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->textacy) (3.2.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (2.4.8)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (0.10.2)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (6.4.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.10.13)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (23.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (4.5.0)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy~=3.0->textacy) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy~=3.0->textacy) (0.1.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy~=3.0->textacy) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy~=3.0->textacy) (2.1.3)\n","Installing collected packages: pyphen, jellyfish, floret, cytoolz, textacy\n","Successfully installed cytoolz-0.12.2 floret-0.10.4 jellyfish-1.0.1 pyphen-0.14.0 textacy-0.13.0\n"]}],"source":["# Установка textacy\n","!pip3 install  textacy"]},{"cell_type":"markdown","metadata":{"id":"qbUeaXxk6xnu"},"source":["Сложные обработки требуют применения сложных моделей, которые основаны на обучаемых нейронных сетях. Следует пользоваться уже обученными под конкретный язык моделями. Загрузим модель для английского языка `en_core_web_lg`."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40383,"status":"ok","timestamp":1697639877488,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"},"user_tz":-180},"id":"meQgmSYHZBww","outputId":"2a0a1e3b-11db-43aa-86b0-bb64b1882cc1"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-10-18 14:37:22.895603: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-10-18 14:37:24.048975: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Collecting en-core-web-lg==3.6.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.6.0/en_core_web_lg-3.6.0-py3-none-any.whl (587.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.6.0) (3.6.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.10)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.10.2)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.66.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.23.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.10.13)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (23.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2023.7.22)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.1.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.1.3)\n","Installing collected packages: en-core-web-lg\n","Successfully installed en-core-web-lg-3.6.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_lg')\n"]}],"source":["# Загрузка модели для анализа английского языка 500 Mb\n","!python3 -m spacy download en_core_web_lg"]},{"cell_type":"markdown","metadata":{"id":"XvS_z8J4b-b-"},"source":["\n","\n","---\n","\n","**После установки необходимо перезапустить среду Colab!** Увы, это ограничение для среды.\n","\n","Зайдите в меню \"Среда выполнения\" и выберите \"Перезапустить среду выполнения\", подтвердите, нажав \"ДА\".\n","\n","После перезапуска можно продолжать работу с кодом ниже.\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yRX6tW6LCtSA"},"source":["Сначала мы подключим библиотеку ```import spacy```\n","\n","Загрузим в память модель для английского языка ``` nlp = spacy.load('en_core_web_lg') ``` .\n","\n","en_core_web_lg это название модели, которая была заранее скачена на диск.\n","\n","Создадим некоторый текст `text`, который будем обрабатывать, можете создать свой.\n","\n","Выполним __парсинг__ текста командой `nlp()`. Парсинг проанализирует текст и определит типы слов\\словосочетаний, к чему они относятся.\n","Например, можем узнать все __именованные сущности__ `ents`: найденные имена собственные, слова (словосочетания) которые указывают на положение объектов, слова которые описывают дату\\время и др. Их много разных типов: https://spacy.io/api/annotation#named-entities .\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UMeh-kSh9B8j"},"source":["Обозначение и смысл их приведен ниже:\n","```\n","PERSON  \tЛюди (имена, фамилии, прозвища и т.п.), в том числе вымышленные.\n","NORP\t    Названия национальных, религиозных, политических объединений.\n","FAC  \t   Названия зданий, аэропортов, шоссе, мостов ...\n","ORG  \t   Названия компаний, агентств, организаций ...\n","GPE  \t   Названия стран, городов, штатов (округов) ...\n","LOC  \t   Названия географических областей (кроме относящихся к GPE), горных массивов, водоемов...\n","PRODUCT\t Названия товаров, автомобилей, еды (кроме услуг) ...\n","EVENT       Названия событий, ураганов, битв, войн, спортивных состязаний...\n","WORK_OF_ART Названия произведений искусств, книг, картин, песен...\n","LAW  \t   Названия юридических документов\n","LANGUAGE\tНазвание языков\n","DATE\t    Указание на даты и периоды, абсолютные или относительные\n","TIME\t    Указание на время (меньше дня)\n","PERCENT\t Указания на проценты ”%“\n","MONEY\t   Указания на деньги, значение и единицы\n","QUANTITY\tУказания на количество чего-либо, вес, размер, ....\n","ORDINAL\t Указание на порядок “первый”, “второй”, и так далее\n","CARDINAL\tЧисла, которые не попали в другие категории\n","```\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":11403,"status":"ok","timestamp":1697640028855,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"},"user_tz":-180},"id":"j56CtQt_CtSA"},"outputs":[],"source":["import spacy # подключим библиотеку\n","\n","# Загрузим NLP-модель для английского языка\n","nlp = spacy.load('en_core_web_lg') # en_core_web_lg это название модели, которая была скачена и установлена\n","\n","# Текст для анализа. Можете написать свой текст (на английском)\n","text = \"\"\"London is the capital and most populous city of England and\n","the United Kingdom.  Standing on the River Thames in the south east\n","of the island of Great Britain, London has been a major settlement\n","for two millennia. It was founded by the Romans, who named it Londinium.\n","\"\"\"\n","\n","# Парсинг текста с помощью spaCy. Эта команда запускает целый конвейер по обработке текста\n","doc = nlp(text)\n"]},{"cell_type":"code","source":["doc.ents"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v6Mj0f7TPBHp","executionInfo":{"status":"ok","timestamp":1697640140282,"user_tz":-180,"elapsed":340,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"}},"outputId":"d0ace0f3-648b-4c40-b43c-a7ec157d6a24"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(London,\n"," England,\n"," the United Kingdom,\n"," the River Thames,\n"," south east,\n"," Great Britain,\n"," London,\n"," two millennia,\n"," Romans,\n"," Londinium)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":343,"status":"ok","timestamp":1697640252189,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"},"user_tz":-180},"id":"lA_kzyGe9TTa","outputId":"98099599-03b6-4740-cd75-1f8344732cc7"},"outputs":[{"output_type":"stream","name":"stdout","text":["London (GPE)\n","England (GPE)\n","the United Kingdom (GPE)\n","the River Thames (LOC)\n","south east (LOC)\n","Great Britain (GPE)\n","London (GPE)\n","two millennia (DATE)\n","Romans (NORP)\n","Londinium (GPE)\n"]}],"source":["# в переменной 'doc' теперь содержится обработанная версия текста\n","# мы можем делать с ней все что угодно!\n","# например, распечатать все обнаруженные именованные сущности (в .ents)\n","for entity in doc.ents:\n","    print(f\"{entity.text} ({entity.label_})\")# печатаем слово .text (словосочетание) и его тип .label_\n"]},{"cell_type":"markdown","metadata":{"id":"z-gBAzqvCtSA"},"source":["\n","\n","Попробуйте обработать свой текст. Для этого или исправьте текст выше или загрузите свой - вы уже умеете это делать."]},{"cell_type":"markdown","metadata":{"id":"0xqAJ8BNCtSB"},"source":["# Обработка текста модулем `nltk`\n","Библиотека `nltk` также предназначена для работы с текстом на естественном языке.\n","Документацию смотрите на https://www.nltk.org/ . Библиотеку необходимо установить заранее.\n","\n","Сама библиотека лишь предоставляет функции для обработки, правила обработки, какой вид обработки делать, зависит от *модели* которая будет использоваться.\n","Один из видов обработки это __токенизация__.\n","\n","\n","Токенизация (иногда – сегментация) - это разбиение текста на части, по словам, по предложениям и т.п.\n","\n","Для английского языка для токенизации мы загрузим модель `'punkt'` с помощью функции `nltk.download()`. Модель уже была обучена на большом количестве текстов, чтобы правильно проводить токенизацию."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3001,"status":"ok","timestamp":1697640746394,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"},"user_tz":-180},"id":"g6muYl8kCtSB","outputId":"1d71794e-cc41-4711-e67c-33a81e44e999"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}],"source":["import nltk\n","nltk.download('punkt')\n","#import nltk.data"]},{"cell_type":"markdown","metadata":{"id":"cvcS7DFiCtSC"},"source":["## Токенизация по предложениям\n","\n","Токенизация по предложениям – это процесс разделения письменного языка на предложения-компоненты. В английском и некоторых других языках мы можем вычленять предложение каждый раз, когда находим определенный знак пунктуации – точку.\n","\n","Но даже в английском эта задача нетривиальна, так как точка используется и в сокращениях. Таблица сокращений может сильно помочь во время обработки текста, чтобы избежать неверной расстановки границ предложений. В большинстве случаев для этого используются библиотеки, так что можете особо не переживать о деталях реализации."]},{"cell_type":"markdown","metadata":{"id":"drmfU9a4CtSC"},"source":["Чтобы сделать токенизацию предложений с помощью NLTK, можно воспользоваться методом `nltk.sent_tokenize`\n","\n","Конечно модели могут и ошибаться, попробуйте сделать текст, чтобы обмануть токенайзер."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":227,"status":"ok","timestamp":1697640895804,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"},"user_tz":-180},"id":"-sjjoONECtSC","outputId":"286ee40b-841a-4640-b277-115e4fd04cd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["We try to implement NLTK.Sent_tokenize.\n","\n","It is very hard to produce good tokens.\n","\n","Our approach is model-based one!\n","\n","And they has already train a good model for tokenizing.\n","\n","Really?\n","\n","Yes ... try.\n","\n","Hard words: vice president, half sister\n","\n"]}],"source":["text = \"We try to implement NLTK.Sent_tokenize. It is very hard to produce good tokens. Our approach is model-based one! And they has already train a good model for tokenizing. Really? Yes ... try. Hard words: vice president, half sister\"\n","sentences = nltk.sent_tokenize(text)\n","for sentence in sentences:\n","    print(sentence)\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gf0Dsg6eHRKH"},"outputs":[],"source":["#text = \"sentences = nltk.sent_tokenize(text) for sentence in sentences:     print(sentence )     print()\"\n","#sentences = nltk.sent_tokenize(text)\n","#for sentence in sentences:\n","#    print(sentence)\n","#    print()"]},{"cell_type":"markdown","metadata":{"id":"PcaU-EJmCtSC"},"source":["### Токенизация по словам\n","\n","Токенизация (иногда – сегментация) по словам – это процесс разделения предложений на слова-компоненты. В английском и многих других языках, использующих ту или иную версию латинского алфавита, пробел – это неплохой разделитель слов.\n","\n","Тем не менее, могут возникнуть проблемы, если мы будем использовать только пробел – в английском составные существительные пишутся по-разному и иногда через пробел. И тут вновь нам помогают библиотеки."]},{"cell_type":"markdown","metadata":{"id":"7HeF9J6YCtSC"},"source":["Возьмем предложения из предыдущего блока кода и применим к каждому из них метод `word_tokenize()`"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":243,"status":"ok","timestamp":1697641244315,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"},"user_tz":-180},"id":"C87AGUEuCtSC","outputId":"cee1109f-16f5-43db-d38f-d7731994fa4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["['We', 'try', 'to', 'implement', 'NLTK.Sent_tokenize', '.']\n","\n","['It', 'is', 'very', 'hard', 'to', 'produce', 'good', 'tokens', '.']\n","\n","['Our', 'approach', 'is', 'model-based', 'one', '!']\n","\n","['And', 'they', 'has', 'already', 'train', 'a', 'good', 'model', 'for', 'tokenizing', '.']\n","\n","['Really', '?']\n","\n","['Yes', '...', 'try', '.']\n","\n","['Hard', 'words', ':', 'vice', 'president', ',', 'half', 'sister']\n","\n"]}],"source":["for sentence in sentences:\n","    words = nltk.word_tokenize(sentence)\n","    print(words)\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"v0tMvd88CtSD"},"source":["### Лемматизация и стемминг текста"]},{"cell_type":"markdown","metadata":{"id":"LeRcCpI_CtSD"},"source":["Обычно тексты содержат разные грамматические формы одного и того же слова, а также могут встречаться однокоренные слова. Лемматизация и стемминг преследуют цель привести все встречающиеся словоформы к одной, нормальной словарной форме."]},{"cell_type":"markdown","metadata":{"id":"Fgyf8zxSCtSD"},"source":["Лемматизация и стемминг – это частные случаи нормализации и они отличаются.\n","\n","Стемминг – это грубый эвристический процесс, который отрезает «лишнее» от корня слов, часто это приводит к потере словообразовательных суффиксов.\n","\n","Лемматизация – это более тонкий процесс, который использует словарь и морфологический анализ, чтобы в итоге привести слово к его канонической форме – лемме.\n","\n","Отличие в том, что стеммер (конкретная реализация алгоритма стемминга) действует без знания контекста и, соответственно, не понимает разницу между словами, которые имеют разный смысл в зависимости от части речи. Однако у стеммеров есть и свои преимущества: их проще внедрить и они работают быстрее. Плюс, более низкая «аккуратность» может не иметь значения в некоторых случаях."]},{"cell_type":"markdown","metadata":{"id":"S0H8wT73CtSD"},"source":["Примеры:\n","\n","Слово good – это лемма для слова better. Стеммер не увидит эту связь, так как здесь нужно сверяться со словарем.\n","Слово play – это базовая форма слова playing. Тут справятся и стемминг, и лемматизация.\n","Слово meeting может быть как нормальной формой существительного, так и формой глагола to meet, в зависимости от контекста. В отличие от стемминга, лемматизация попробует выбрать правильную лемму, опираясь на контекст.\n","\n","Теперь, когда мы знаем, в чем разница, давайте рассмотрим пример.\n","\n","Сначала мы подключим модель стемминга  `PorterStemmer` и лемматизации `WordNetLemmatizer`, загрузим и подключим \"корпус\" `wordnet`  в котором много слов, синонимов и т.п. (чтобы не путаться в версиях, скачаем всё).\n","\n","Создадим функцию `compare_stemmer_and_lemmatizer()`, которая будет приводить слова к нормальной форме. Ей мы укажем каким стеммером и леммером (есть такое слово, а?) пользоваться, само слово, которое надо лемматизировать, и часть речи `POS` желаемого результата (`VERB`  - глагол, `NOUN` - существительное).\n","```\n","Обозначение \t Значение \t    Примеры\n","ADJ              прилагательное  new, good, high, special, big, local\n","ADP              предлог         on, of, at, with, by, into, under\n","ADV              наречие         really, already, still, early, now\n","CONJ             союз            and, or, but, if, while, although\n","DET              артикль,        \n","                 определитель    the, a, some, most, every, no, which\n","NOUN             существительное year, home, costs, time, Africa\n","NUM              числительное    twenty-four, fourth, 1991, 14:24\n","PRT              частица         at, on, out, over per, that, up, with\n","PRON             местоимение     he, their, her, its, my, I, us\n","VERB             глагол          is, say, told, given, playing, would\n",".                знак пунктуации . , ; !\n","X                другое          ersatz, esprit, dunno, gr8, univeristy\n","\n","```\n","Стеммер вызывается функцией `stem(word)`, ему все-равно какая часть речи должна получиться.\n","\n","Леммер вызывается функцией `lemmatize(word,pos)`, ему мы указываем и слово и желаемую часть речи.\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48724,"status":"ok","timestamp":1697641543804,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"},"user_tz":-180},"id":"RdOS2LcPCtSE","outputId":"11217ea0-4072-4473-9568-ba793bf680f0"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/alpino.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n","[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/crubadan.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dolch.zip.\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n","[nltk_data]    | Downloading package extended_omw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/floresta.zip.\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/indian.zip.\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/kimmo.zip.\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/moses_sample.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/paradigms.zip.\n","[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pe08.zip.\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/perluniprops.zip.\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pil.zip.\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pl196x.zip.\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ptb.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Package punkt is already up-to-date!\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/qc.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/rslp.zip.\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/rte.zip.\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/smultron.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/switchboard.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet.zip.\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ycoe.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"]}],"source":["from nltk.stem import PorterStemmer, WordNetLemmatizer\n","#nltk.download('wordnet')\n","nltk.download('all') # скачаем все....\n","from nltk.corpus import wordnet\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":441,"status":"ok","timestamp":1697641592016,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"},"user_tz":-180},"id":"CpEKfwAeCqaB"},"outputs":[],"source":["# Сравнение стеммера и леммера\n","def compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word, pos):\n","    \"\"\"\n","    Print the results of stemming and lemmitization using the passed stemmer, lemmatizer, word and pos (part of speech)\n","    \"\"\"\n","    print(\"Word:\", word)\n","    print(\"Stemmer:\", stemmer.stem(word))\n","    print(\"Lemmatizer:\", lemmatizer.lemmatize(word, pos))\n","    print()\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3868,"status":"ok","timestamp":1697641599551,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"},"user_tz":-180},"id":"5LVaZD-fCjEL","outputId":"067e93db-4b0d-496e-d734-cd3fef3ea2bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Word: seeking\n","Stemmer: seek\n","Lemmatizer: seek\n","\n","Word: drove\n","Stemmer: drove\n","Lemmatizer: drive\n","\n","Word: meeting\n","Stemmer: meet\n","Lemmatizer: meeting\n","\n","Word: meeting\n","Stemmer: meet\n","Lemmatizer: meet\n","\n"]}],"source":["lemmatizer = WordNetLemmatizer()\n","stemmer = PorterStemmer()\n","compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = \"seeking\", pos = wordnet.VERB)\n","compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = \"drove\", pos = wordnet.VERB)\n","compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = \"meeting\", pos = wordnet.NOUN)\n","compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = \"meeting\", pos = wordnet.VERB)"]},{"cell_type":"markdown","metadata":{"id":"Ul0lfY0CCtSE"},"source":["### Стоп-слова"]},{"cell_type":"markdown","metadata":{"id":"ZqQZyagsCtSE"},"source":["Стоп-слова – это слова, которые выкидываются из текста до/после обработки текста. Когда мы применяем машинное обучение к текстам, такие слова могут добавить много шума, поэтому необходимо избавляться от ненужных слов.\n","\n","Стоп-слова это обычно артикли, междометия, союзы и т.д., которые не несут смысловой нагрузки. При этом надо понимать, что не существует универсального списка стоп-слов, все зависит от конкретного случая.\n","\n","В NLTK есть предустановленный список стоп-слов. Перед первым использованием вам понадобится его скачать: `nltk.download(\"stopwords\")`. После скачивания можно подключить модуль `stopwords` и посмотреть на сами слова:"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":242,"status":"ok","timestamp":1697641890979,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"},"user_tz":-180},"id":"uBioI1CNCtSE","outputId":"4cefff63-a6c9-469f-d5bd-afdd0213f649"},"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"]}],"source":["#nltk.download(\"stopwords\") # уже скачали все\n","from nltk.corpus import stopwords\n","print(stopwords.words(\"english\"))"]},{"cell_type":"markdown","metadata":{"id":"vcENi1jcCtSE"},"source":["Рассмотрим, как можно убрать стоп-слова из предложения:"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":216,"status":"ok","timestamp":1697641930482,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"},"user_tz":-180},"id":"aUuHyTkvCtSF","outputId":"99f6eabe-afd4-433b-c6fe-71fd1cc3ccda"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Backgammon', 'one', 'oldest', 'known', 'board', 'games', '.']\n"]}],"source":["stop_words = set(stopwords.words(\"english\")) # получим стоп-слова, превратим их в множество с помощью set()\n","sentence = \"Backgammon is one of the oldest known board games.\" # зададим строку\n","\n","words = nltk.word_tokenize(sentence) # токенизируем ее по словам\n","\n","# и будем в цикле перебирать все слова из words, проверять входит ли оно в множество стоп-слов stop_words,\n","# и если нет, то вернем слово word, иначе ничего не вернем.\n","without_stop_words = [word for word in words if not word in stop_words]\n","print(without_stop_words) #\n"]},{"cell_type":"markdown","metadata":{"id":"cVm_-ALhCtSH"},"source":["## Дополнительное чтение\n","\n","Дополнительно почитайте самостоятельно про __регулярные выражения__\n"," https://habr.com/ru/post/349860/\n","\n","и про парсер для русского языка `Natasha`. С его помощью можно вытаскивать некоторые именованные сущности на русском языке из текста для дальнейшего анализа и обработки, такие как адреса, даты, имена.\n","https://habr.com/ru/post/349864/\n","\n","Посмотрите на полезный список инструментов для обработки естественных языков\n","\n","Там указано, для каких задач они предназначены и для каких языков.\n","https://nlpub.mipt.ru/Обработка_текста\n","\n","Сейчас мы не будем про это говорить, но, когда будем обрабатывать тексты - еще вернемся.  "]},{"cell_type":"markdown","metadata":{"id":"yLgcfKZlCtSH"},"source":["## Домашнее задание"]},{"cell_type":"markdown","metadata":{"id":"fTThqiIPCtSH"},"source":["Написать на английском языке сочинение на 1 страницу о том, как вы провели лето и определить все именованные сущности и их тип в этом тексте.\n","\n","В тексте должно быть указано, где вы были, когда и с кем.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xbEXE9rtCtSH"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"nbTranslate":{"displayLangs":["*"],"hotkey":"alt-t","langInMainMenu":true,"sourceLang":"en","targetLang":"fr","useGoogleTranslate":true},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":0}